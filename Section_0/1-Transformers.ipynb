{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Transformers Library by Hugging Face\n",
    "\n",
    "The **Transformers** library by Hugging Face provides **pre-trained models** and tools for a wide range of AI tasks, including:\n",
    "\n",
    "- **Natural Language Processing (NLP)**: Text classification, translation, question-answering, and more.\n",
    "- **Support for Popular Models**: Includes BERT, GPT, LLaMA, and many others.\n",
    "- **Framework Integration**: Works seamlessly with **PyTorch** and **TensorFlow**.\n",
    "\n",
    "A powerful tool to accelerate and simplify your machine learning projects! ðŸš€\n",
    "\n",
    "# first install transfomers\n",
    "\n",
    "- **!pip install Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9933708906173706}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\",device='cuda')\n",
    "classifier(\"This book is not so good but it is not bad either\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **using model and tokenizer directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1424,  0.1335, -0.1291,  ..., -0.3597, -0.0562,  0.3605],\n",
      "         [-0.3506,  0.1042,  0.6244,  ..., -0.1761,  0.4834,  0.0644],\n",
      "         [-0.2451, -0.1573,  0.6945,  ..., -0.5654, -0.0894, -0.1856],\n",
      "         [-0.8248, -0.9119, -0.6561,  ...,  0.5074, -0.1939, -0.1659],\n",
      "         [ 0.8767,  0.0352, -0.1233,  ...,  0.2720, -0.6369, -0.1585]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.9756e-01, -3.3040e-01, -7.6942e-01,  7.5799e-01,  4.6678e-01,\n",
      "         -1.2035e-01,  9.1835e-01,  1.8087e-01, -7.2716e-01, -9.9991e-01,\n",
      "         -4.4723e-01,  8.9104e-01,  9.6621e-01,  5.4915e-01,  9.4344e-01,\n",
      "         -7.6605e-01, -6.0469e-01, -6.1654e-01,  4.0572e-01, -7.4644e-01,\n",
      "          6.1739e-01,  9.9974e-01,  3.2989e-02,  2.5414e-01,  4.3106e-01,\n",
      "          9.7732e-01, -8.4328e-01,  9.2297e-01,  9.4871e-01,  6.3994e-01,\n",
      "         -7.3620e-01,  9.0957e-02, -9.7607e-01, -1.9115e-01, -8.1717e-01,\n",
      "         -9.7907e-01,  3.1635e-01, -7.2013e-01,  1.6124e-01,  5.1998e-02,\n",
      "         -8.9102e-01,  2.3002e-01,  9.9967e-01,  1.1504e-02,  1.1176e-01,\n",
      "         -3.5556e-01, -1.0000e+00,  2.8487e-01, -8.3339e-01,  8.0987e-01,\n",
      "          7.4664e-01,  6.2762e-01,  1.9847e-01,  4.5185e-01,  4.7242e-01,\n",
      "         -3.9172e-03, -1.0570e-01,  9.3340e-02, -1.8906e-01, -5.1946e-01,\n",
      "         -5.7823e-01,  2.8046e-01, -8.2281e-01, -8.9764e-01,  9.1890e-01,\n",
      "          7.4117e-01, -5.9993e-02, -3.1655e-01,  2.8223e-03, -1.4652e-01,\n",
      "          9.0953e-01,  2.5587e-01,  5.4492e-02, -8.4187e-01,  5.7772e-01,\n",
      "          2.4804e-01, -6.1732e-01,  1.0000e+00, -6.4891e-01, -9.5425e-01,\n",
      "          5.5482e-01,  6.8866e-01,  5.5711e-01, -3.3463e-01,  4.7904e-01,\n",
      "         -1.0000e+00,  3.4198e-01, -1.1332e-01, -9.7818e-01,  2.2888e-01,\n",
      "          4.5161e-01, -1.5720e-01,  1.6196e-01,  5.1240e-01, -5.7139e-01,\n",
      "         -3.7296e-01, -2.8502e-01, -8.0263e-01, -2.1937e-01, -2.2406e-01,\n",
      "         -5.1867e-03, -2.6417e-01, -2.2839e-01, -3.7414e-01,  2.6933e-01,\n",
      "         -4.5077e-01, -5.5173e-01,  4.8430e-01,  4.7179e-02,  6.6727e-01,\n",
      "          3.3201e-01, -2.9447e-01,  5.0653e-01, -9.4933e-01,  6.1857e-01,\n",
      "         -2.4586e-01, -9.7565e-01, -5.3396e-01, -9.7924e-01,  5.8934e-01,\n",
      "         -2.3402e-01, -2.7537e-01,  9.4687e-01,  1.5282e-01,  3.0224e-01,\n",
      "          1.1186e-02, -7.3235e-01, -1.0000e+00, -6.9951e-01, -5.0531e-01,\n",
      "         -2.7527e-01, -2.0676e-01, -9.5607e-01, -9.1109e-01,  5.1232e-01,\n",
      "          9.3519e-01,  1.3547e-01,  9.9877e-01, -2.0620e-01,  9.2237e-01,\n",
      "         -4.9195e-01, -7.0814e-01,  6.5991e-01, -4.0702e-01,  7.4080e-01,\n",
      "          4.4237e-01, -6.9697e-01,  1.7959e-01, -2.0311e-01,  2.8031e-01,\n",
      "         -6.5004e-01, -2.2036e-01, -6.8982e-01, -9.2266e-01, -3.2942e-01,\n",
      "          9.3754e-01, -4.7618e-01, -8.9325e-01, -1.4759e-01, -1.4892e-01,\n",
      "         -5.3520e-01,  8.4738e-01,  7.0655e-01,  3.8489e-01, -4.0430e-01,\n",
      "          3.9196e-01,  2.2351e-01,  5.5196e-01, -8.1159e-01, -1.8874e-01,\n",
      "          3.3696e-01, -3.6883e-01, -6.5580e-01, -9.7058e-01, -3.4790e-01,\n",
      "          4.8676e-01,  9.8414e-01,  7.1667e-01,  2.3032e-01,  7.0561e-01,\n",
      "         -8.2328e-02,  7.3910e-01, -9.2162e-01,  9.5893e-01, -3.3638e-01,\n",
      "          2.3401e-01,  6.1109e-02,  4.1919e-01, -8.5576e-01,  1.5276e-01,\n",
      "          8.8090e-01, -6.3311e-01, -8.3712e-01,  6.5170e-02, -4.3054e-01,\n",
      "         -3.4557e-01, -5.9076e-01,  5.1503e-01, -2.6207e-01, -2.4693e-01,\n",
      "          6.1307e-02,  8.7691e-01,  9.8752e-01,  8.0232e-01,  1.4752e-01,\n",
      "          6.8812e-01, -8.9320e-01, -5.4703e-01,  4.8356e-02,  2.1540e-01,\n",
      "          2.3905e-01,  9.9014e-01, -4.9880e-01, -1.3789e-01, -9.2269e-01,\n",
      "         -9.7721e-01, -9.4687e-02, -9.0299e-01, -1.3468e-01, -7.0078e-01,\n",
      "          5.1664e-01,  2.4403e-01,  6.1268e-01,  3.6200e-01, -9.9236e-01,\n",
      "         -7.4132e-01,  3.4922e-01, -2.3057e-01,  3.8191e-01, -1.6780e-01,\n",
      "          3.9557e-02,  8.9809e-01, -5.3350e-01,  8.3722e-01,  8.8839e-01,\n",
      "         -7.2514e-01, -7.3965e-01,  8.9407e-01, -2.3408e-01,  8.7030e-01,\n",
      "         -5.6373e-01,  9.7425e-01,  8.4706e-01,  8.3271e-01, -8.9631e-01,\n",
      "         -5.6269e-01, -9.0159e-01, -6.9973e-01,  6.1689e-02,  9.8561e-02,\n",
      "          8.7749e-01,  5.6404e-01,  3.4049e-01,  3.7582e-01, -6.7834e-01,\n",
      "          9.9827e-01, -3.3602e-02, -9.2002e-01,  2.7571e-01, -3.0865e-01,\n",
      "         -9.6896e-01,  7.2166e-01,  3.3953e-01, -4.0723e-03, -4.1497e-01,\n",
      "         -6.4642e-01, -9.2770e-01,  9.3160e-01,  6.0920e-02,  9.9071e-01,\n",
      "          4.6195e-02, -9.3994e-01, -6.6822e-01, -8.9785e-01, -3.0716e-01,\n",
      "         -2.0500e-01, -3.8715e-01, -8.1207e-02, -9.4563e-01,  4.3475e-01,\n",
      "          4.1566e-01,  4.9148e-01, -6.9445e-01,  9.9830e-01,  1.0000e+00,\n",
      "          9.3747e-01,  8.9366e-01,  9.2651e-01, -9.9799e-01, -2.9389e-01,\n",
      "          9.9995e-01, -9.7730e-01, -1.0000e+00, -9.1234e-01, -6.9568e-01,\n",
      "          3.7277e-01, -1.0000e+00, -2.0609e-01,  1.7125e-01, -8.7448e-01,\n",
      "          5.9413e-01,  9.6720e-01,  9.9095e-01, -1.0000e+00,  6.5990e-01,\n",
      "          9.2647e-01, -6.0201e-01,  9.5347e-01, -3.2178e-01,  9.5794e-01,\n",
      "          6.5932e-01,  1.1768e-01, -2.4565e-01,  3.2490e-01, -8.8230e-01,\n",
      "         -8.7855e-01, -4.5894e-01, -6.2306e-01,  9.8882e-01,  1.1127e-01,\n",
      "         -8.1504e-01, -8.9768e-01, -2.5662e-02, -2.0069e-01, -3.3576e-01,\n",
      "         -9.4615e-01, -1.0597e-01,  5.6197e-01,  7.7034e-01,  9.0876e-02,\n",
      "          3.0440e-01, -6.8938e-01,  2.5243e-01, -1.7008e-01,  3.3678e-01,\n",
      "          6.2607e-01, -9.2541e-01, -6.4668e-01, -4.1812e-01, -1.5776e-01,\n",
      "         -5.8090e-01, -9.3592e-01,  9.5067e-01, -4.7707e-01,  7.9794e-01,\n",
      "          1.0000e+00,  1.2838e-01, -8.6789e-01,  6.2421e-01,  2.0469e-01,\n",
      "          4.3401e-03,  1.0000e+00,  7.7308e-01, -9.5914e-01, -4.9383e-01,\n",
      "          5.1447e-01, -5.0643e-01, -4.6023e-01,  9.9656e-01, -2.7889e-01,\n",
      "         -6.0374e-01, -3.1379e-01,  9.4870e-01, -9.7579e-01,  9.8235e-01,\n",
      "         -8.9237e-01, -9.4238e-01,  9.3981e-01,  9.0497e-01, -6.8348e-01,\n",
      "         -4.2984e-01,  1.2406e-01, -6.3880e-01,  3.0334e-01, -9.6749e-01,\n",
      "          7.3108e-01,  5.1503e-01,  4.4069e-02,  8.4879e-01, -9.0855e-01,\n",
      "         -4.6638e-01,  2.9909e-01, -7.3568e-01, -2.1431e-01,  7.7439e-01,\n",
      "          5.1014e-01, -3.0549e-01,  9.3043e-02, -3.4041e-01,  2.4432e-01,\n",
      "         -9.6357e-01,  3.4927e-01,  1.0000e+00, -2.4683e-01,  4.8418e-01,\n",
      "         -5.1474e-01,  5.1816e-02, -1.7388e-01,  4.7761e-01,  5.5869e-01,\n",
      "         -2.4163e-01, -8.0916e-01,  7.3723e-01, -9.7524e-01, -9.7020e-01,\n",
      "          8.5261e-01,  1.8727e-01, -2.7090e-01,  9.9998e-01,  4.6983e-01,\n",
      "          9.6979e-02,  3.6852e-01,  9.7414e-01, -8.3735e-04,  6.4376e-01,\n",
      "          8.7053e-01,  9.6059e-01, -1.8756e-01,  5.1957e-01,  8.7386e-01,\n",
      "         -8.3668e-01, -2.7663e-01, -5.9559e-01, -1.2338e-02, -8.8570e-01,\n",
      "          8.2443e-02, -9.3376e-01,  9.5201e-01,  8.6130e-01,  3.3720e-01,\n",
      "          2.3967e-01,  5.4095e-01,  1.0000e+00,  1.3089e-01,  7.5866e-01,\n",
      "         -6.5313e-01,  9.1302e-01, -9.9812e-01, -8.6791e-01, -3.1752e-01,\n",
      "         -9.1609e-03, -7.3271e-01, -3.1350e-01,  2.6582e-01, -9.5944e-01,\n",
      "          7.4126e-01,  4.9215e-01, -9.9320e-01, -9.8625e-01, -2.1465e-01,\n",
      "          9.0425e-01, -4.9062e-02, -9.3739e-01, -7.3878e-01, -5.6868e-01,\n",
      "          5.5593e-01, -2.0358e-01, -9.3490e-01, -1.9005e-01, -2.3025e-01,\n",
      "          4.5899e-01, -7.0444e-02,  5.4063e-01,  7.6846e-01,  5.9393e-01,\n",
      "         -1.9536e-01, -1.4414e-01, -2.6515e-02, -8.3050e-01,  8.8996e-01,\n",
      "         -8.3560e-01, -8.2514e-01, -1.7632e-01,  1.0000e+00, -5.3209e-01,\n",
      "          7.4747e-01,  7.9880e-01,  8.0815e-01, -1.0571e-01,  9.6487e-02,\n",
      "          8.7753e-01,  2.1673e-01, -7.5100e-01, -8.1242e-01, -9.0104e-01,\n",
      "         -3.2831e-01,  6.3693e-01,  8.2366e-03,  5.5376e-01,  7.1281e-01,\n",
      "          6.4975e-01,  1.4424e-01,  3.0208e-02, -1.5367e-01,  9.9958e-01,\n",
      "         -2.4823e-01, -4.5267e-02, -4.9861e-01,  8.4448e-03, -3.3764e-01,\n",
      "         -7.4455e-01,  1.0000e+00,  2.2909e-01,  3.0400e-01, -9.8225e-01,\n",
      "         -7.7581e-01, -9.3825e-01,  9.9999e-01,  8.0493e-01, -6.9461e-01,\n",
      "          6.7583e-01,  7.1560e-01, -7.5956e-02,  8.8280e-01, -8.8797e-02,\n",
      "         -3.6776e-01,  3.0358e-01,  9.0578e-02,  9.3570e-01, -5.4713e-01,\n",
      "         -9.4064e-01, -5.1854e-01,  3.6407e-01, -9.4586e-01,  9.9884e-01,\n",
      "         -4.6634e-01, -2.2036e-01, -4.2909e-01,  1.9812e-01,  8.0296e-01,\n",
      "         -1.1237e-01, -9.7593e-01,  3.2541e-03,  2.2387e-01,  9.4407e-01,\n",
      "          2.3152e-01, -5.3741e-01, -9.0152e-01,  7.2972e-01,  6.4679e-01,\n",
      "         -8.5261e-01, -9.2280e-01,  9.4217e-01, -9.8578e-01,  6.6755e-01,\n",
      "          1.0000e+00,  3.4225e-01, -1.6761e-01,  7.7094e-02, -3.9836e-01,\n",
      "          2.2625e-01, -2.2041e-01,  7.3612e-01, -9.2743e-01, -3.6491e-01,\n",
      "         -1.7540e-01,  2.9468e-01, -1.1303e-01,  2.6401e-01,  6.9200e-01,\n",
      "          1.5430e-01, -4.0083e-01, -5.2640e-01,  2.4087e-02,  4.3949e-01,\n",
      "          8.6077e-01, -2.6866e-01, -1.0388e-01,  7.5590e-03, -1.2241e-01,\n",
      "         -9.1376e-01, -2.1348e-01, -2.9673e-01, -9.9988e-01,  6.9455e-01,\n",
      "         -1.0000e+00,  2.8368e-01,  8.6736e-02, -1.2220e-01,  7.9206e-01,\n",
      "          1.5490e-02,  4.9311e-01, -7.3105e-01, -8.1444e-01,  5.0827e-01,\n",
      "          7.2358e-01, -2.9349e-01, -5.1790e-01, -6.9773e-01,  2.4637e-01,\n",
      "         -1.5527e-02,  2.1339e-01, -5.1806e-01,  7.8137e-01, -1.7301e-01,\n",
      "          1.0000e+00,  1.9527e-01, -7.5410e-01, -9.8262e-01,  1.5416e-01,\n",
      "         -2.1457e-01,  1.0000e+00, -9.3285e-01, -9.1573e-01,  2.9430e-01,\n",
      "         -6.9617e-01, -8.3260e-01,  2.6820e-01, -3.4682e-02, -7.8862e-01,\n",
      "         -8.5820e-01,  9.5101e-01,  9.4690e-01, -5.5200e-01,  4.5879e-01,\n",
      "         -3.4441e-01, -5.6058e-01, -4.5146e-02,  7.0248e-01,  9.7133e-01,\n",
      "          2.6254e-01,  8.9137e-01,  4.2302e-01, -7.3707e-02,  9.4970e-01,\n",
      "          1.6445e-01,  6.1381e-01,  1.0270e-01,  1.0000e+00,  2.7983e-01,\n",
      "         -8.8676e-01,  1.4404e-01, -9.7244e-01, -1.6864e-01, -9.5221e-01,\n",
      "          2.6231e-01,  2.7997e-01,  8.9316e-01, -2.3383e-01,  9.4295e-01,\n",
      "         -5.3833e-01,  4.3594e-02, -7.9243e-01, -3.1933e-01,  3.5616e-01,\n",
      "         -9.0270e-01, -9.7206e-01, -9.7301e-01,  6.8435e-01, -4.2106e-01,\n",
      "          4.4723e-02,  1.2440e-01,  7.1746e-03,  3.5916e-01,  4.4393e-01,\n",
      "         -1.0000e+00,  9.2962e-01,  3.8557e-01,  8.5492e-01,  9.2992e-01,\n",
      "          7.0625e-01,  4.8370e-01,  2.3270e-01, -9.7541e-01, -9.8218e-01,\n",
      "         -3.2932e-01, -2.1163e-01,  7.5084e-01,  6.3202e-01,  8.8856e-01,\n",
      "          4.3251e-01, -5.0404e-01, -6.7029e-02, -2.9505e-01, -3.2807e-01,\n",
      "         -9.8568e-01,  4.3447e-01, -5.8658e-01, -9.7724e-01,  9.3651e-01,\n",
      "         -1.3708e-01, -1.3665e-01, -9.2905e-02, -6.6635e-01,  9.7278e-01,\n",
      "          7.0707e-01,  4.3204e-01,  8.7915e-02,  4.7217e-01,  8.3990e-01,\n",
      "          9.4993e-01,  9.7219e-01, -6.7202e-01,  8.3279e-01, -5.4167e-01,\n",
      "          4.1126e-01,  3.4719e-01, -9.0791e-01,  9.1833e-02,  2.0506e-01,\n",
      "         -2.8610e-01,  1.5352e-01, -1.1304e-01, -9.8395e-01,  1.8631e-01,\n",
      "         -2.0863e-01,  6.1166e-01, -2.9382e-01,  5.8578e-02, -3.7076e-01,\n",
      "         -3.7402e-02, -6.2659e-01, -7.5935e-01,  5.6508e-01,  5.0258e-01,\n",
      "          8.7703e-01,  8.0735e-01, -6.9027e-02, -6.4556e-01, -2.2308e-01,\n",
      "         -7.0203e-01, -8.9239e-01,  9.4453e-01, -2.7452e-02, -3.4130e-01,\n",
      "          5.0977e-01, -1.5005e-01,  5.0155e-01,  9.2493e-02, -3.1371e-01,\n",
      "         -3.9861e-01, -6.5197e-01,  8.3609e-01,  4.2015e-02, -5.3505e-01,\n",
      "         -7.0400e-01,  5.6731e-01,  3.1090e-01,  9.9974e-01, -6.7131e-01,\n",
      "         -8.3116e-01, -1.6593e-01, -3.9197e-01,  2.8166e-01, -4.2960e-01,\n",
      "         -1.0000e+00,  3.9846e-01, -3.7210e-01,  6.3236e-01, -6.8413e-01,\n",
      "          4.8827e-01, -7.2434e-01, -9.8220e-01, -1.6253e-01,  4.2168e-02,\n",
      "          6.8659e-01, -4.7086e-01, -7.6191e-01,  4.8094e-01, -1.6689e-01,\n",
      "          9.5317e-01,  8.1554e-01, -3.6865e-01,  1.1301e-01,  6.0376e-01,\n",
      "         -6.5595e-01, -6.1998e-01,  9.0952e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "Last Hidden State Shape :  torch.Size([1, 5, 768])\n",
      "Last Hidden State Size: 3840\n",
      "Pooler Output Shape: torch.Size([1, 768])\n",
      "Pooler Output Size: 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ArithmeticError outputs is last hidden states and pooler output of the model(embeddings of the input text) \\n(The pooler_output is not token-level information but a summary representation of the entire input.))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "txt = \"Hello world!\"\n",
    "\n",
    "inp = tokenizer(txt,return_tensors=\"pt\") # return_tensors=\"pt\" returns PyTorch tensors and \"tf\" returns TensorFlow tensors\n",
    "out= model(**inp)\n",
    "print(out) \n",
    "print(\"Last Hidden State Shape : \",out.last_hidden_state.shape) # (batch_size, sequence_length, hidden_size) *sequence_length is the number of tokens in the input text\n",
    "print(\"Last Hidden State Size:\", out.last_hidden_state.numel())  # Total elements\n",
    "\n",
    "# If pooler_output is available\n",
    "if hasattr(out, \"pooler_output\"):\n",
    "    print(\"Pooler Output Shape:\", out.pooler_output.shape)\n",
    "    print(\"Pooler Output Size:\", out.pooler_output.numel())\n",
    "\"\"\"ArithmeticError outputs is last hidden states and pooler output of the model(embeddings of the input text) \n",
    "(The pooler_output is not token-level information but a summary representation of the entire input.))\n",
    "\"\"\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9997029900550842}\n",
      "{'label': 'NEGATIVE', 'score': 0.9987803101539612}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\" # for other models, you could search in the Hugging Face model hub\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline (\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "results = classifier ([\"we love Automns\",\n",
    "                       \"i dont hate it but it is bad\",])\n",
    "for result in results:\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check what Tokenizer is doing with data.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens: ['we', 'are', 'very', 'happy', 'to', 'show', 'you', 'the', 'transformers', 'library', '.']\n",
      "##############################\n",
      "Token IDs: [2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 19081, 3075, 1012]\n",
      "##############################\n",
      "Input IDs: {'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "##############################\n",
      "{'input_ids': tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996, 19081,\n",
      "          3075,  1012,   102],\n",
      "        [  101,  2057,  3246,  2017,  2123,  1005,  1056,  5223,  2009,  1012,\n",
      "           102,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "##############################\n",
      "batch shape: torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name) # pay attention that we use AutoModelForSequenceClassification instead of AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "tokens = tokenizer.tokenize(\"We are very happy to show you the Transformers library.\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids (tokens)\n",
    "input_ids = tokenizer (\"We are very happy to show you the Transformers library.\")\n",
    "input_ids = tokenizer (\"We are very happy to show you the Transformers library.\")\n",
    "\n",
    "print (f' Tokens: {tokens}')\n",
    "print (\"###\"*10)\n",
    "print (f'Token IDs: {token_ids}') # the diffrence between **token_ids** and **input_ids** is that token_ids are the ids of the tokens and input_ids are the ids of the tokens with special tokens like [CLS] and [SEP] and padding tokens\n",
    "print (\"###\"*10)\n",
    "print (f'Input IDs: {input_ids}') #attention_mask means hast same length with input_ids and 1 for real tokens and 0 for padding tokens that means the 1 means the models should pay attention to the token and 0 means the model should ignore the token.\n",
    "\n",
    "X_train = [\"We are very happy to show you the Transformers library.\",\n",
    "            \"We hope you don't hate it.\"]\n",
    "batch = tokenizer (X_train, padding=True, truncation=True, # batch is a dictionary that contains input_ids, attention_mask, token_type_ids. this batch encoding is useful for training the model. padding is for padding the input_ids to the same Length  when the input_ids are not the same length. truncation is for truncating the input_ids to the same length when the input_ids are not the same length that is because the model can't handle the input_ids with different lengths.\n",
    "                   max_length=512, return_tensors=\"pt\")\n",
    "print (\"###\"*10)\n",
    "print(batch) \n",
    "print (\"###\"*10)\n",
    "print(f'batch shape: {batch[\"input_ids\"].shape}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Tokenizer and Model separately and inference the Model using pytorch.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996, 19081,\n",
      "          3075,  1012,   102],\n",
      "        [  101,  2057,  3246,  2017,  2123,  1005,  1056,  5223,  2009,  1012,\n",
      "           102,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "##############################\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.1329,  4.3811],\n",
      "        [ 0.0818, -0.0418]]), hidden_states=None, attentions=None)\n",
      "##############################\n",
      "tensor([[2.0060e-04, 9.9980e-01],\n",
      "        [5.3086e-01, 4.6914e-01]])\n",
      "##############################\n",
      "tensor([1, 0])\n",
      "##############################\n",
      "['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "X_train = [\"We are very happy to show you the Transformers library.\",\n",
    "            \"We hope you don't hate it.\"]\n",
    "batch = tokenizer(X_train, padding=True, truncation=True,\n",
    "                   max_length=512, return_tensors=\"pt\")\n",
    "print(batch)\n",
    "print (\"###\"*10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    print (\"###\"*10)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)\n",
    "    print (\"###\"*10)\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)\n",
    "    print (\"###\"*10)\n",
    "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
