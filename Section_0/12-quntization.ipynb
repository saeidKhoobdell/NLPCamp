{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **bitsandbytes**\n",
    "bitsandbytes enables accessible large language models via k-bit quantization for PyTorch. bitsandbytes provides three main features for dramatically reducing memory consumption for inference and training.\n",
    "([official Documentation](https://huggingface.co/docs/bitsandbytes/main/en/index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,pipeline\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\"load_in_4bit\": True} # This is the magic line\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"[INST] <>\n",
    "You are a helpful, respectful and honest assistant. Answer exactly in few words.\n",
    "<>\n",
    "\n",
    "Answer the question below:\n",
    "\n",
    "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
    "[/INST]\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt_template,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=4, # Number of responses to generate\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(\"*****\"*50)\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
